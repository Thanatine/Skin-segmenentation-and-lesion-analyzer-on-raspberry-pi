{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "tf.random.set_seed(101)\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = '../../Documents/skin-cancer-mnist-ham10000/'\n",
    "# base_dir = 'base_dir'\n",
    "# train_dir = os.path.join(base_dir, 'train_dir')\n",
    "# val_dir = os.path.join(base_dir, 'val_dir')\n",
    "\n",
    "# df_data = pd.read_csv(dataset_dir + '/HAM10000_metadata.csv')\n",
    "\n",
    "# # this will tell us how many images are associated with each lesion_id\n",
    "# df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# # now we filter out lesion_id's that have only one image associated with it\n",
    "# df = df[df['image_id'] == 1]\n",
    "# df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# # here we identify lesion_id's that have duplicate images and those that have only\n",
    "# # one image.\n",
    "\n",
    "# def identify_duplicates(x):\n",
    "#     unique_list = list(df['lesion_id'])\n",
    "    \n",
    "#     if x in unique_list:\n",
    "#         return 'no_duplicates'\n",
    "#     else:\n",
    "#         return 'has_duplicates'\n",
    "    \n",
    "# # create a new colum that is a copy of the lesion_id column\n",
    "# df_data['duplicates'] = df_data['lesion_id']\n",
    "# # apply the function to this new column\n",
    "# df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n",
    "\n",
    "# # now we filter out images that don't have duplicates\n",
    "# df = df_data[df_data['duplicates'] == 'no_duplicates']\n",
    "\n",
    "# # now we create a val set using df because we are sure that none of these images\n",
    "# # have augmented duplicates in the train set\n",
    "# y = df['dx']\n",
    "\n",
    "# _, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n",
    "\n",
    "# # This set will be df_data excluding all rows that are in the val set\n",
    "\n",
    "# # This function identifies if an image is part of the train\n",
    "# # or val set.\n",
    "# def identify_val_rows(x):\n",
    "#     # create a list of all the lesion_id's in the val set\n",
    "#     val_list = list(df_val['image_id'])\n",
    "    \n",
    "#     if str(x) in val_list:\n",
    "#         return 'val'\n",
    "#     else:\n",
    "#         return 'train'\n",
    "\n",
    "# # identify train and val rows\n",
    "\n",
    "# # create a new colum that is a copy of the image_id column\n",
    "# df_data['train_or_val'] = df_data['image_id']\n",
    "# # apply the function to this new column\n",
    "# df_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n",
    "   \n",
    "# # filter out train rows\n",
    "# df_train = df_data[df_data['train_or_val'] == 'train']\n",
    "\n",
    "\n",
    "# print(len(df_train))\n",
    "# print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for v2\n",
    "dataset_dir = '../../Documents/skin-cancer-mnist-ham10000/'\n",
    "base_dir = 'base_dir_v2'\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "\n",
    "df_data = pd.read_csv(dataset_dir + '/HAM10000_metadata.csv')\n",
    "\n",
    "\n",
    "#X = df_data.drop('dx', axis=1)\n",
    "y = df_data['dx']\n",
    "\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.1, random_state=101, stratify=y)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path = base_dir + '/train_dir'\n",
    "valid_path = base_dir + '/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "image_size = 224\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "train_batches = datagen.flow_from_directory(train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=train_batch_size)\n",
    "\n",
    "valid_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=val_batch_size)\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=False)\n",
    "\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def top_2_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "\n",
    "class_weights = {\n",
    "    0: 1.0, # akiec\n",
    "    1: 1.0, # bcc\n",
    "    2: 1.0, # bkl\n",
    "    3: 1.0, # df\n",
    "    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n",
    "    5: 1.0, # nv\n",
    "    6: 1.0, # vasc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a copy of a mobilenet model\n",
    "mobile = tf.keras.applications.mobilenet.MobileNet()\n",
    "\n",
    "mobile.summary()\n",
    "print(type(mobile.layers))\n",
    "# How many layers does MobileNet have?\n",
    "print(len(mobile.layers))\n",
    "\n",
    "# CREATE THE MODEL ARCHITECTURE\n",
    "\n",
    "# Exclude the last 5 layers of the above model.\n",
    "# This will include all layers up to and including global_average_pooling2d_1\n",
    "x = mobile.layers[-6].output\n",
    "\n",
    "# Create a new dense layer for predictions\n",
    "# 7 corresponds to the number of classes\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n",
    "# dense layer we created above.\n",
    "\n",
    "model = Model(inputs=mobile.input, outputs=predictions)\n",
    "\n",
    "# We need to choose how many layers we actually want to be trained.\n",
    "\n",
    "# Here we are freezing the weights of all layers except the\n",
    "# last 23 layers in the new model.\n",
    "# The last 23 layers of the model will be trained.\n",
    "\n",
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n",
    "              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n",
    "\n",
    "filepath = \"mobilenet_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                              class_weight=class_weights,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=30, verbose=1,\n",
    "                   callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the the last epoch will be used.\n",
    "\n",
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "\n",
    "model.load_weights('mobilenet_model.h5')\n",
    "\n",
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the loss and accuracy curves\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_top2_acc = history.history['top_2_accuracy']\n",
    "val_top2_acc = history.history['val_top_2_accuracy']\n",
    "train_top3_acc = history.history['top_3_accuracy']\n",
    "val_top3_acc = history.history['val_top_3_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training cat acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n",
    "plt.title('Training and validation cat accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\n",
    "plt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\n",
    "plt.title('Training and validation top2 accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\n",
    "plt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\n",
    "plt.title('Training and validation top3 accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mobilenet_v2(last_layers_training=-1):\n",
    "    IMG_SHAPE = (224, 224, 3)\n",
    "\n",
    "    # Create the base model from the pre-trained model MobileNet V2\n",
    "    mobilev2 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                   weights='imagenet')\n",
    "#     mobilev2.summary()\n",
    "#     print(len(mobilev2.layers))\n",
    "\n",
    "    x = mobilev2.layers[-2].output\n",
    "    predictions = Dense(7, activation='softmax')(x)\n",
    "    model = Model(inputs=mobilev2.input, outputs=predictions)\n",
    "\n",
    "    for layer in model.layers[:last_layers_training]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    base_learning_rate = 0.0001\n",
    "\n",
    "    model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n",
    "                  metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n",
    "\n",
    "    filepath = \"mobilenetv2_model_\" + str(last_layers_training) + \".h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                 save_best_only=True, mode='max')\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=2, \n",
    "                                       verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "\n",
    "    callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "    history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                                  class_weight=class_weights,\n",
    "                        validation_data=valid_batches,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=20, verbose=1,\n",
    "                       callbacks=callbacks_list)\n",
    "    \n",
    "    # Here the best epoch will be used.\n",
    "    model.load_weights(filepath)\n",
    "\n",
    "    val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "    model.evaluate_generator(test_batches, \n",
    "                            steps=len(df_val))\n",
    "\n",
    "    print('val_loss:', val_loss)\n",
    "    print('val_cat_acc:', val_cat_acc)\n",
    "    print('val_top_2_acc:', val_top_2_acc)\n",
    "    print('val_top_3_acc:', val_top_3_acc)\n",
    "\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_top2_acc = history.history['top_2_accuracy']\n",
    "    val_top2_acc = history.history['val_top_2_accuracy']\n",
    "    train_top3_acc = history.history['top_3_accuracy']\n",
    "    val_top3_acc = history.history['val_top_3_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training cat acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n",
    "    plt.title('Training and validation cat accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "    plt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\n",
    "    plt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\n",
    "    plt.title('Training and validation top2 accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\n",
    "    plt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\n",
    "    plt.title('Training and validation top3 accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mobilenet_v2(last_layers_training=-5) # lock until last 5 layers\n",
    "# train_mobilenet_v2(last_layers_training=-13) # lock until last 5 layers + block_16\n",
    "\n",
    "# for i in range(1, 10): # lock until last 5 layers + block_16 + block_15 ~ block_6  \n",
    "#     train_mobilenet_v2(last_layers_training=-13-9*i)\n",
    "\n",
    "# for i in range(10, 16):\n",
    "#     train_mobilenet_v2(last_layers_training=-13-9*i)\n",
    "    \n",
    "# train_mobilenet_v2(last_layers_training=-157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MobileNet_V3 import build_mobilenet_v3\n",
    "model = build_mobilenet_v3(input_size=224, num_classes=7, model_type='small', pooling_type='avg', include_top=True)\n",
    "\n",
    "print(model.summary())\n",
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "\n",
    "model.compile(Adam(lr=0.1), loss='categorical_crossentropy', \n",
    "              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n",
    "\n",
    "filepath = \"mobilenetv3_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                              class_weight=class_weights,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=80, verbose=1,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "\n",
    "model.load_weights(filepath)\n",
    "\n",
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)\n",
    "\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_top2_acc = history.history['top_2_accuracy']\n",
    "val_top2_acc = history.history['val_top_2_accuracy']\n",
    "train_top3_acc = history.history['top_3_accuracy']\n",
    "val_top3_acc = history.history['val_top_3_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training cat acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n",
    "plt.title('Training and validation cat accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\n",
    "plt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\n",
    "plt.title('Training and validation top2 accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\n",
    "plt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\n",
    "plt.title('Training and validation top3 accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
